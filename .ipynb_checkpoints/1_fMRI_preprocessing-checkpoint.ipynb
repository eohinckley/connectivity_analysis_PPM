{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resting state fMRI preprocessing\n",
    "This notebook contains preprocessing tailored precision mapping in children. \n",
    "\n",
    "### 1. Unwarping \n",
    "* Measure distortion with the fieldmap\n",
    "* Eliminate variance (motion, deformations,interactions)\n",
    "\n",
    "### 2. General fMRI processing\n",
    "* Slice-time correction\n",
    "* Rigid realignment (also extract DVARS, FD, motion params)\n",
    "* Co-registration to the sMRI (T1-weighted structural MRI resampled to 2x2x2mm voxels)\n",
    "\n",
    "### 3. Resting state processing\n",
    "* Derive temporal mask based on FD and DVARS\n",
    "* De-noise to remove:\n",
    "    - Noise associated with white matter and CSF- delete the GM and smooth what is left\n",
    "    - Noise associated with background signal - delete brain and smooth what's left\n",
    "    - Global signal\n",
    "    - motion regressors\n",
    "    - Motion derivatives (lagged 8 times)\n",
    "    - Motion spikes (FD>0.2mm, DVARS>2 SDs from mean)\n",
    "* Bandpass filter\n",
    "* delete high motion timepoints and concatenate runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "import graphviz\n",
    "from os import listdir, makedirs\n",
    "from os.path import isdir\n",
    "from nipype.interfaces.io import DataSink, SelectFiles, DataGrabber, FreeSurferSource # Data i/o\n",
    "from nipype.interfaces.utility import IdentityInterface, Function     # utility\n",
    "from nipype.pipeline.engine import Node, Workflow, MapNode, JoinNode        # pypeline engine\n",
    "from nipype.interfaces.nipy.preprocess import Trim\n",
    "from nipype.interfaces.ants import N4BiasFieldCorrection\n",
    "from nipype.interfaces.fsl import SliceTimer, MCFLIRT, FLIRT, BET, Merge\n",
    "from nipype.interfaces.fsl.utils import Reorient2Std, MotionOutliers\n",
    "from nipype.interfaces.fsl.maths import ApplyMask, MeanImage\n",
    "from nipype.interfaces.freesurfer import Resample, Binarize, BBRegister, MRIConvert\n",
    "from nipype.algorithms.confounds import CompCor\n",
    "from nipype.interfaces.afni.preprocess import Bandpass\n",
    "from nipype.interfaces.afni.utils import AFNItoNIFTI\n",
    "from pandas import DataFrame, read_csv\n",
    "\n",
    "#set output file type for FSL to NIFTI_GZ\n",
    "from nipype.interfaces.fsl.preprocess import FSLCommand\n",
    "FSLCommand.set_default_output_type('NIFTI_GZ')\n",
    "\n",
    "# MATLAB setup - Specify path to current SPM and the MATLAB's default mode\n",
    "from nipype.interfaces.matlab import MatlabCommand\n",
    "MatlabCommand.set_default_paths('~/spm12/toolbox')\n",
    "MatlabCommand.set_default_matlab_cmd(\"matlab -nodesktop -nosplash\")\n",
    "\n",
    "# Set study variables\n",
    "study_home = '/Users/SEAlab/Documents/PPM'\n",
    "raw_data =  '/Users/SEAlab/Documents/PPM/Data'\n",
    "output_dir = study_home + '/fMRIproc/preprocessing'\n",
    "\n",
    "workflow_dir = study_home + '/Workflows'\n",
    "custom_timings = study_home + '/Misc/slice_timing.txt'\n",
    "fs_dir = study_home + '/fMRIproc/freesurfer'\n",
    "session_info = read_csv(study_home + '/Misc/session_info.csv',index_col=None)\n",
    "session_info = session_info.astype({'subject_id':str})\n",
    "subject_ids = session_info['subject_id'].unique().tolist()\n",
    "#subject_ids = list(map(str,subject_ids))\n",
    "#subject_ids = ['5000']\n",
    "\n",
    "proc_cores = 6 # number of cores of processing for the workflows\n",
    "\n",
    "interleave = True\n",
    "TR = 1.1 # in seconds\n",
    "slice_dir = 3 # 1=x, 2=y, 3=z\n",
    "resampled_voxel_size = (2,2,2)\n",
    "\n",
    "fd_threshold = 0.2 #in mm\n",
    "dvars_threshold = 2 # in standard units\n",
    "\n",
    "highpass_freq = 0.008 #in Hz\n",
    "lowpass_freq = 0.09 #in Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File handling Nodes\n",
    "\n",
    "# Identity node for each subject\n",
    "subinfosource = Node(IdentityInterface(fields=['subject_id']),\n",
    "                     name='subinfosource')\n",
    "subinfosource.iterables = [('subject_id', subject_ids)]\n",
    "\n",
    "# identity node for each file\n",
    "funcinfosource = Node(IdentityInterface(fields=['subject_id','filename']),\n",
    "                     name='subinfosource')\n",
    "funcinfosource.iterables = [('subject_id', session_info['subject_id'].tolist()),\n",
    "                            ('filename',session_info['run_name'].to_list())]\n",
    "funcinfosource.synchronize=True\n",
    "\n",
    "# Datasink- where our select outputs will go\n",
    "substitutions = [('_subject_id_', '')]\n",
    "datasink = Node(DataSink(), name='datasink')\n",
    "datasink.inputs.base_directory = output_dir\n",
    "datasink.inputs.container = output_dir\n",
    "datasink.inputs.substitutions = substitutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Unwarping \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_pes(pes):\n",
    "    from nipype import config, logging\n",
    "    from nipype.interfaces.fsl import Merge\n",
    "    from os.path import abspath\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    \n",
    "    print(pes)\n",
    "    pe1s = []\n",
    "    pe0s = []\n",
    "    for file in pes:\n",
    "        if 'Fsp' in file:\n",
    "            pe0s.append(file)\n",
    "        elif 'Fsa' in file:\n",
    "            pe1s.append(file)\n",
    "    \n",
    "    if len(pe1s) < len(pe0s):\n",
    "        for a in range(len(pe1s),len(pe0s)):\n",
    "            pe1s.append(pe1s[0])\n",
    "    \n",
    "    pe1s = sorted(pe1s)\n",
    "    pe0s = sorted(pe0s)\n",
    "\n",
    "    me = Merge()\n",
    "    merged_pes = []\n",
    "    \n",
    "    for i in range(0,len(pe1s)):\n",
    "        num=pe1s[i][-12:-11]\n",
    "        me.inputs.in_files = [pe1s[i],pe0s[i]]\n",
    "        me.inputs.dimension='t'\n",
    "        me.inputs.merged_file = 'merged_pes%s.nii.gz' % num\n",
    "        me.run()\n",
    "        file = abspath('merged_pes%s.nii.gz' % num)\n",
    "        merged_pes.append(file)\n",
    "        \n",
    "    return(merged_pes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pes_template = {'pes': raw_data + '/%s/*rsEPI_Fs*.nii.gz'}\n",
    "select_pes = Node(DataGrabber(sort_filelist=True,\n",
    "                              template = raw_data + '/%s/*rsEPI_Fs*.nii.gz',\n",
    "                              field_template = pes_template,\n",
    "                              base_directory=raw_data,\n",
    "                              infields=['subject_id'], \n",
    "                              template_args={'pes':[['subject_id']]}), \n",
    "                  name='select_pes')\n",
    "\n",
    "func_template = {'func': raw_data + '/%s/*rsEPI_Fsp*.nii.gz'}\n",
    "select_func = Node(DataGrabber(sort_filelist=True,\n",
    "                               template = raw_data + '/%s/*rsEPI_Fsp*.nii.gz',\n",
    "                               field_template = func_template,\n",
    "                               base_directory=raw_data,\n",
    "                               infields=['subject_id'], \n",
    "                               template_args={'func':[['subject_id']]}), \n",
    "                   name='select_func')\n",
    "\n",
    "# include only the first volume of each PE volume\n",
    "trim_PEs = MapNode(ExtractROI(t_min=0, t_size=5),name='trim_PEs', \n",
    "                   iterfield=['in_file'])\n",
    "\n",
    "sort_pe_list = Node(Function(input_names=['pes'],\n",
    "                             output_names=['merged_pes'],\n",
    "                             function=sort_pes), \n",
    "                    name='sort_pe_list')\n",
    "\n",
    "topup = MapNode(TOPUP(encoding_file=phase_encoding_file), name='topup',iterfield=['in_file'])\n",
    "\n",
    "apply_topup = MapNode(ApplyTOPUP(in_index=[6], encoding_file=phase_encoding_file,\n",
    "                                 method='jac', out_corrected='func_unwarped.nii.gz'),\n",
    "                      name='apply_topup',iterfield=['in_topup_fieldcoef','in_topup_movpar','in_files'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepreprocflow = Workflow(name='prepreprocflow')\n",
    "prepreprocflow.connect([(infosource,select_pes, [('subject_id','subject_id')]),\n",
    "                        (infosource,select_func, [('subject_id','subject_id')]),\n",
    "                        (select_pes,trim_PEs, [('pes','in_file')]), \n",
    "                        (trim_PEs,sort_pe_list, [('roi_file','pes')]),\n",
    "                        (sort_pe_list,topup, [('merged_pes','in_file')]),\n",
    "                        (topup, apply_topup, [('out_fieldcoef','in_topup_fieldcoef'), \n",
    "                                              ('out_movpar','in_topup_movpar')]),\n",
    "                        (select_func, apply_topup, [('func','in_files')]),\n",
    "                        (apply_topup, datasink, [('out_corrected','unwarped_funcs')])\n",
    "                       ])\n",
    "\n",
    "prepreprocflow.base_dir = workflow_dir\n",
    "prepreprocflow.write_graph(graph2use='flat')\n",
    "prepreprocflow.run('MultiProc', plugin_args={'n_procs': proc_cores, 'memory_gb':20})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process T1 anat for fMRI processing\n",
    "1. Resample skullstripped T1w anat\n",
    "2. Resample segmentation \n",
    "3. Create tissue masks:\n",
    "    - GM only\n",
    "    - Non-GM only\n",
    "    - Full brain (no ventricles)\n",
    "    - Full brain (with ventricles and dilated)\n",
    "    - Extra-cerebral space\n",
    "4. Resample surfaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_masks_freesurfer(aseg_nifti):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from numpy import zeros\n",
    "    from nipype.interfaces.freesurfer import Binarize\n",
    "    from os.path import abspath\n",
    "    \n",
    "    # load the subject's segmentation volumes\n",
    "    aseg_img = load(aseg_nifti)\n",
    "    aseg_data = aseg_img.get_data()\n",
    "    \n",
    "    # list indices for each tissue type (from freesurfer aseg)\n",
    "    wm = [2, 7, 41, 46, 60, 28, 16, 77, 251, 252, 253, 254, 255]\n",
    "    gm = [3, 6, 8, 9, 10, 11, 12, 13, 17, 18, 26, 42, 47, 49, 50, 51, 52, 53, 54, 58]\n",
    "    csf = [4, 5, 14, 15, 24, 63]\n",
    "    non_gm = wm + csf\n",
    "    whole_brain = wm + gm + csf\n",
    "    \n",
    "    # preallocate zeros for the tissue masks\n",
    "    wm_combined = zeros(aseg_data.shape)\n",
    "    gm_combined = zeros(aseg_data.shape)\n",
    "    csf_combined = zeros(aseg_data.shape)\n",
    "    non_gm_combined = zeros(aseg_data.shape)\n",
    "    whole_brain_combined = zeros(aseg_data.shape)\n",
    "    \n",
    "    # put labels into lists to make looping easier\n",
    "    tissues_idx = [wm, gm, csf, non_gm, whole_brain]\n",
    "    tissues_data = [wm_combined, gm_combined, csf_combined, non_gm_combined, whole_brain_combined]\n",
    "    tissue_name = ['wm','gm','csf','non_gm','whole_brain']\n",
    "    \n",
    "    # initialize the binarize class\n",
    "    binarize = Binarize()\n",
    "    binarize.inputs.min = 0.5\n",
    "    binarize.inputs.dilate = 1\n",
    "    binarize.inputs.erode = 1\n",
    "    \n",
    "    # isolate labels for specific regions\n",
    "    for x in range(0,5):\n",
    "        for label in tissues_idx[x]:\n",
    "            tissues_data[x][aseg_data==label] = 1\n",
    "        tmp_img = Nifti1Image(tissues_data[x],header=aseg_img.header, affine=aseg_img.affine)\n",
    "        save(tmp_img, tissue_name[x] + '_mask.nii.gz')\n",
    "         \n",
    "        binarize.inputs.in_file = abspath(tissue_name[x] + '_mask.nii.gz')\n",
    "        binarize.inputs.binary_file = tissue_name[x] + '_mask.nii.gz'\n",
    "        binarize.run()\n",
    "    \n",
    "    # make dilated brain mask\n",
    "    binarize.inputs.in_file = abspath('whole_brain_mask.nii.gz')\n",
    "    binarize.inputs.binary_file = 'whole_brain_D1_mask.nii.gz'\n",
    "    binarize.inputs.dilate = 1\n",
    "    binarize.inputs.erode = 0\n",
    "    binarize.run()\n",
    "    \n",
    "    # gather up the mask filepaths to return\n",
    "    wm_csf_mask = abspath('non_gm_mask.nii.gz')\n",
    "    brain_mask = abspath('whole_brain_mask.nii.gz')\n",
    "    gm_only_mask = abspath('gm_mask.nii.gz')\n",
    "    brain_mask_dilated = abspath('whole_brain_D1_mask.nii.gz')\n",
    "    \n",
    "    return(wm_csf_mask, brain_mask, gm_only_mask, brain_mask_dilated)\n",
    "\n",
    "def invert_masks(mask_file):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nibabel import load, save, Nifti1Image\n",
    "    from os.path import abspath, basename\n",
    "    from numpy import zeros\n",
    "    \n",
    "    # load mask data\n",
    "    mask_img = load(mask_file)\n",
    "    mask_data = mask_img.get_data()\n",
    "    \n",
    "    # preallocate new mask data\n",
    "    inv_data = zeros(mask_data.shape)\n",
    "    \n",
    "    # invert the 1s and 0s\n",
    "    inv_data[mask_data==0] = 1\n",
    "    \n",
    "    #save as a new file\n",
    "    inv_img = Nifti1Image(inv_data, header = mask_img.header, affine = mask_img.affine)\n",
    "    save(inv_img, 'inverted_' + basename(mask_file))\n",
    "    inverted_mask = abspath('inverted_' + basename(mask_file))\n",
    "    \n",
    "    return(inverted_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FreeSurferSource node to grab processed T1w data from freesurfer\n",
    "t1w_source = Node(FreeSurferSource(subjects_dir=fs_dir), name='t1w_source')\n",
    "\n",
    "# resample brain file\n",
    "resample_anat = Node(MRIConvert(vox_size=resampled_voxel_size, \n",
    "                                out_type='niigz', out_file='resampled_anat.nii.gz'), name='resample_anat')\n",
    "\n",
    "# resample segmentation file\n",
    "resample_seg = Node(MRIConvert(vox_size=resampled_voxel_size, \n",
    "                               resample_type='nearest', \n",
    "                               out_type='niigz'), name='resample_seg')\n",
    "\n",
    "# make masks\n",
    "make_masks = Node(Function(input_names=['aseg_nifti'], \n",
    "                           output_names=['wm_csf_mask', 'brain_mask', 'gm_only_mask', 'brain_mask_dilated'], \n",
    "                           function=make_masks_freesurfer), name='make_masks')\n",
    "\n",
    "#invert the brain mask\n",
    "invert_mask = Node(Function(input_names=['mask_file'], \n",
    "                            output_names=['inverted_mask'], \n",
    "                            function=invert_masks), name='invert_mask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "anatpreproc = Workflow(name='fmri_anat_preprocflow')\n",
    "anatpreproc.connect([(subinfosource, t1w_source, [('subject_id','subject_id')]),\n",
    "                     (t1w_source, resample_anat, [('brain','in_file')]),\n",
    "                     (t1w_source, resample_seg, [('aseg','in_file')]),\n",
    "                     (resample_seg, make_masks, [('out_file','aseg_nifti')]),\n",
    "                     (make_masks, invert_mask, [('brain_mask','mask_file')]),\n",
    "                     \n",
    "                     (make_masks, datasink, [('wm_csf_mask','wm_csf_mask'),\n",
    "                                             ('brain_mask','brain_mask'),\n",
    "                                             ('gm_only_mask','gm_mask'),\n",
    "                                             ('brain_mask_dilated','brain_mask_dilated')]),\n",
    "                     (resample_anat, datasink, [('out_file','resampled_t1w_anat')]),\n",
    "                     (invert_mask, datasink, [('inverted_mask','background_mask')])\n",
    "                    ])\n",
    "anatpreproc.base_dir = workflow_dir\n",
    "#anatpreproc.write_graph(graph2use='flat')\n",
    "anatpreproc.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess fMRI resting state data\n",
    "These nodes and workflow (preprocflow) perform basic preprocessing to align the functional volumes into a common space.\n",
    "1. Reorient images to standard space\n",
    "2. Reslice the structural image to 2mm isotropic\n",
    "3. Functional image slice time correction\n",
    "4. Rigid realignment to middle volume of functional image\n",
    "5. Coregistration of functional images to structural image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## File handling Nodes\n",
    "substitutions = [('_subject_id_', '_'),('_filename_','')]\n",
    "datasink.inputs.substitutions = substitutions\n",
    "\n",
    "# Select anat\n",
    "anat_template = {'brain_mask': output_dir + '/brain_mask_dilated/{subject_id}/whole_brain_D1_mask.nii.gz',\n",
    "                 'resliced_anat': output_dir + '/resampled_t1w_anat/{subject_id}/resampled_anat.nii.gz'}\n",
    "selectanat = Node(SelectFiles(anat_template), name='selectanat')\n",
    "\n",
    "# Data grabber- select fMRI\n",
    "func_template = {'func':raw_data + '/{subject_id}/FUNC_NIFTIS/{filename}_unwarped.nii.gz'}\n",
    "selectfunc = Node(SelectFiles(func_template), name='selectfunc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_func(template,subject_id,session_info):\n",
    "    func = []\n",
    "    filelist=session_info[session_info['subject_id']==int(subject_id)]['run_name'].to_list()\n",
    "    for f in filelist:\n",
    "        func.append(template.format(subject_id, f))\n",
    "        \n",
    "    print(func)\n",
    "    return(func)\n",
    "\n",
    "def norm_timeseries(in_file, mask_file):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    from nilearn.masking import apply_mask, unmask\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    raw_data = apply_mask(in_file, mask_file)\n",
    "    scaler = StandardScaler(with_mean=True)\n",
    "    norm_data = scaler.fit_transform(raw_data)\n",
    "    img = unmask(norm_data, mask_file)\n",
    "    img.to_filename('norm_func.nii.gz')\n",
    "    out_file = abspath('norm_func.nii.gz')\n",
    "    return(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Nodes for preprocessing\n",
    "\n",
    "#Slice timing correction based on interleaved acquisition using FSL\n",
    "slicetime_correct = Node(SliceTimer(interleaved=interleave, \n",
    "                                    custom_timings=custom_timings,\n",
    "                                    time_repetition=TR, \n",
    "                                    out_file='st_func.nii.gz'),\n",
    "                         name='slicetime_correct')\n",
    "# Rigid realignment\n",
    "realign = Node(MCFLIRT(out_file='rest_moco.nii.gz',save_plots=True), \n",
    "               name='realign')\n",
    "\n",
    "# compute DVARS and FD\n",
    "calc_dvars = Node(MotionOutliers(out_metric_values='out_mot_metric.txt',\n",
    "                                 out_metric_plot='plot.png', metric='dvars'),\n",
    "                     name='calc_dvars')\n",
    "\n",
    "calc_fd = Node(MotionOutliers(out_metric_values='out_mot_metric.txt',\n",
    "                              out_metric_plot='plot.png', metric='fd'),\n",
    "               name='calc_fd')\n",
    "\n",
    "# Registration- using bbregister\n",
    "coreg = Node(BBRegister(contrast_type='bold', out_fsl_file=True,\n",
    "                        subjects_dir=fs_dir, registered_file='warped_func.nii.gz'), \n",
    "             name='coreg', iterfield=['source_file'])\n",
    "\n",
    "apply_registration = Node(FLIRT(apply_xfm=True, out_file='reg_func.nii.gz'), \n",
    "                          name='apply_registration')\n",
    "\n",
    "# Resample functional\n",
    "resample_func = Node(MRIConvert(vox_size=resampled_voxel_size, \n",
    "                                out_type='niigz', out_file='func.nii.gz'), \n",
    "                     name='resample_func', iterfield=['in_file'])\n",
    "\n",
    "# normalize data within each run\n",
    "normalize = Node(Function(input_names=['in_file','mask_file'], \n",
    "                          output_names=['out_file'], \n",
    "                          function=norm_timeseries), \n",
    "                 name='normalize')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Preprocessing Workflow\n",
    "fmripreproc = Workflow(name='fmri_preprocflow')\n",
    "fmripreproc.connect([(funcinfosource,selectanat,[('subject_id','subject_id')]), \n",
    "                     (funcinfosource,selectfunc,[('subject_id','subject_id'),\n",
    "                                                 ('filename','filename')]), \n",
    "                     (funcinfosource,coreg,[('subject_id','subject_id')]), \n",
    "                     \n",
    "                     (selectfunc,slicetime_correct,[('func','in_file')]),\n",
    "                     (slicetime_correct,realign,[('slice_time_corrected_file','in_file')]),\n",
    "                     (slicetime_correct,calc_dvars,[('slice_time_corrected_file','in_file')]),\n",
    "                     (slicetime_correct,calc_fd,[('slice_time_corrected_file','in_file')]),\n",
    "                     (realign,apply_registration,[('out_file','in_file')]),\n",
    "                     (realign,coreg, [('out_file','source_file')]),\n",
    "                     (selectanat, apply_registration, [('resliced_anat','reference')]),\n",
    "                     (coreg, apply_registration, [('out_fsl_file','in_matrix_file')]),\n",
    "                     \n",
    "                     (apply_registration, normalize,[('out_file','in_file')]),\n",
    "                     (selectanat,normalize,[('brain_mask','mask_file')]),\n",
    "                   \n",
    "                     (realign, datasink,[('par_file','motion_parameters')]),\n",
    "                     (calc_dvars, datasink,[('out_metric_plot','dvars_plot'),\n",
    "                                            ('out_metric_values','dvars_values')]),\n",
    "                     (calc_fd, datasink, [('out_metric_plot','fd_plot'),\n",
    "                                          ('out_metric_values','fd_values')]),\n",
    "                     (normalize, datasink, [('out_file','registered_func')])\n",
    "                    ])\n",
    "fmripreproc.base_dir = workflow_dir\n",
    "#fmripreproc.write_graph(graph2use='flat')\n",
    "fmripreproc.run('MultiProc', plugin_args={'n_procs': proc_cores})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Nuissance Regressors\n",
    "These nodes and workflow creates both the subject specific and general nuissance regressors needed for preprocessing the rest data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data grabber\n",
    "selectfiles_template = {'brain_mask':output_dir + '/brain_mask/{subject_id}/whole_brain_mask.nii.gz',\n",
    "                        'nonbrain_mask': output_dir + '/background_mask/{subject_id}/inverted_whole_brain_mask.nii.gz', \n",
    "                        'nongm_mask':output_dir + '/wm_csf_mask/{subject_id}/non_gm_mask.nii.gz', \n",
    "                        'func': output_dir + '/registered_func/{filename}_{subject_id}/norm_func.nii.gz', \n",
    "                        'motion': output_dir + '/motion_parameters/{filename}_{subject_id}/rest_moco.nii.gz.par',\n",
    "                        'fd': output_dir + '/fd_values/{filename}_{subject_id}/out_mot_metric.txt',\n",
    "                        'dvars': output_dir + '/dvars_values/{filename}_{subject_id}/out_mot_metric.txt'}\n",
    "selectfiles = Node(SelectFiles(selectfiles_template), name='selectmask')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_blur_func(mask, in_file):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    from numpy import median, where\n",
    "    from nipype.interfaces.fsl import ApplyMask\n",
    "    from glob import glob\n",
    "    from subprocess import check_call\n",
    "\n",
    "    applymask = ApplyMask()\n",
    "    applymask.inputs.mask_file = mask\n",
    "    applymask.inputs.in_file = in_file\n",
    "    applymask.inputs.out_file = 'masked_file.nii.gz'\n",
    "    applymask.inputs.nan2zeros = True\n",
    "    applymask.run()\n",
    "\n",
    "    masked_file = abspath('masked_file.nii.gz')\n",
    "    \n",
    "    \n",
    "    if 'nonbrain' in mask:\n",
    "        check_call(['gunzip',masked_file])\n",
    "        \n",
    "        from nipype.interfaces.spm import Smooth\n",
    "        smooth = Smooth()\n",
    "        smooth.inputs.in_files = 'masked_file.nii'\n",
    "        smooth.inputs.fwhm = [22,4,4]\n",
    "        smooth.inputs.out_prefix = 'blurred_'\n",
    "        smooth.run()\n",
    "        check_call(['gzip','blurred_masked_file.nii'])\n",
    "        \n",
    "    else:\n",
    "        from nipype.interfaces.fsl import Smooth\n",
    "        smooth = Smooth()\n",
    "        smooth.inputs.in_file = masked_file\n",
    "        smooth.inputs.smoothed_file = 'blurred_masked_file.nii.gz'\n",
    "        smooth.inputs.fwhm = 4\n",
    "        smooth.run()\n",
    "\n",
    "    blurred_masked_file = abspath('blurred_masked_file.nii.gz')\n",
    "\n",
    "    return(blurred_masked_file)\n",
    "\n",
    "def leadlagmatrix(motion_file):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from os.path import abspath\n",
    "    import numpy as np\n",
    "\n",
    "    motion_params = np.loadtxt(motion_file, dtype=float)\n",
    "    trs = motion_params.shape[0]\n",
    "    params = motion_params.shape[1]\n",
    "    num_lags = 6\n",
    "    derivatives = np.gradient(motion_params, axis=0)\n",
    "    leadlagderivs = np.zeros((trs,params*num_lags))\n",
    "    derivativessq = derivatives**2\n",
    "    leadlagderivssq = np.zeros((trs,params*num_lags))\n",
    "\n",
    "    for i in range(0,params):\n",
    "        for j in range(0,num_lags):\n",
    "            leadlagderivs[:,j+num_lags*i] =  np.roll(derivatives[:,i],shift=j, axis=0)\n",
    "            leadlagderivs[:j,j+num_lags*i] = 0\n",
    "\n",
    "    for i in range(0,params):\n",
    "        for j in range(0,num_lags):\n",
    "            leadlagderivssq[:,j+num_lags*i] =  np.roll(derivativessq[:,i],shift=j, axis=0)\n",
    "            leadlagderivssq[:j,j+num_lags*i] = 0\n",
    "\n",
    "    np.savetxt('derivsleadlag.txt', leadlagderivs)\n",
    "    np.savetxt('derivssqleadlag.txt', leadlagderivssq)\n",
    "\n",
    "    leadlagderivsmot = abspath('derivsleadlag.txt')\n",
    "    leadlagderivssqmot = abspath('derivssqleadlag.txt')\n",
    "    \n",
    "    return(leadlagderivsmot, leadlagderivssqmot)\n",
    "\n",
    "# create timeseries of high motion volumes for spike regression\n",
    "def make_spike_reg_matrix(fd,dvars,fd_threshold, dvars_threshold):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    import numpy as np\n",
    "    from os.path import abspath\n",
    "\n",
    "    fd = np.genfromtxt(fd)\n",
    "    dvars = np.genfromtxt(dvars)\n",
    "    dvars = (dvars - np.mean(dvars))/np.std(dvars) # convert to standard units\n",
    "    dvars[0]=0\n",
    "    spike_ts_mat = ((fd>=fd_threshold) | (dvars>=dvars_threshold)).astype(int)\n",
    "    n_spikes = sum(spike_ts_mat)\n",
    "    spike_mat_mat = np.zeros((len(spike_ts_mat),n_spikes))\n",
    "\n",
    "    y=0\n",
    "    for x in range(0,len(spike_ts_mat)):\n",
    "        if spike_ts_mat[x]==1:\n",
    "            spike_mat_mat[x,y] = 1\n",
    "            y=y+1\n",
    "\n",
    "    np.savetxt('spike_matrix.txt',spike_mat_mat)\n",
    "    np.savetxt('spike_timeseries.txt',spike_ts_mat)\n",
    "\n",
    "    spike_matrix = abspath('spike_matrix.txt')\n",
    "    spike_timeseries = abspath('spike_timeseries.txt')\n",
    "    \n",
    "    return(spike_matrix, spike_timeseries)\n",
    "\n",
    "def calc_global_signal(func,mask):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    import numpy as np\n",
    "    from os.path import abspath\n",
    "    from nilearn.masking import apply_mask\n",
    "    \n",
    "    func_data = apply_mask(func,mask)\n",
    "    func_data[func_data==0]=np.nan\n",
    "    globsig = np.nanmean(func_data,axis=1) \n",
    "    np.savetxt('global_signal.txt',globsig)\n",
    "    global_signal = abspath('global_signal.txt')\n",
    "    \n",
    "    return(global_signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get scanner noise\n",
    "session_noise = Node(Function(input_names=['mask','in_file'], \n",
    "                              output_names=['blurred_masked_file'],\n",
    "                              function=mask_blur_func), name='session_noise')\n",
    "\n",
    "# get noise associated with WM and CSF\n",
    "wmcsf_noise = Node(Function(input_names=['mask','in_file'], \n",
    "                            output_names=['blurred_masked_file'],\n",
    "                            function=mask_blur_func), name='wmcsf_noise')\n",
    "\n",
    "# extract components from session nifti\n",
    "comp_session_noise = Node(CompCor(repetition_time=TR,\n",
    "                                  num_components=9,\n",
    "                                  components_file='components.txt'), name='comp_session_noise')\n",
    "\n",
    "# extract components from WM-CSF nifti \n",
    "comp_wmcsf_noise = Node(CompCor(repetition_time=TR, \n",
    "                                num_components=9,\n",
    "                                components_file='components.txt'), name='comp_wmcsf_noise')\n",
    "\n",
    "# prepare leadlag motion and derivatives\n",
    "prep_motion = Node(Function(input_names=['motion_file'], \n",
    "                            output_names=['leadlagderivsmot','leadlagderivssqmot'],\n",
    "                            function=leadlagmatrix), name='prep_motion')\n",
    "\n",
    "# create the despiking matrix and the censor timeseries files\n",
    "identify_spikes = Node(Function(input_names=['fd','dvars','fd_threshold', 'dvars_threshold'], \n",
    "                                output_names=['spike_matrix','spike_timeseries'],\n",
    "                                function=make_spike_reg_matrix), name='identify_spikes')\n",
    "identify_spikes.inputs.fd_threshold=fd_threshold\n",
    "identify_spikes.inputs.dvars_threshold=dvars_threshold\n",
    "\n",
    "# calculate global signal regressor\n",
    "calc_globalsignal = Node(Function(input_names=['func','mask'], \n",
    "                                  output_names=['global_signal'], \n",
    "                                  function=calc_global_signal),name='calc_globalsignal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "create_noise_flow = Workflow(name='create_noise_flow')\n",
    "create_noise_flow.connect([(funcinfosource,selectfiles,[('subject_id','subject_id'),\n",
    "                                                       ('filename','filename')]),\n",
    "                           (selectfiles, wmcsf_noise, [('func','in_file')]),\n",
    "                           (selectfiles, session_noise, [('func','in_file')]),\n",
    "                           (selectfiles, wmcsf_noise, [('nongm_mask','mask')]),\n",
    "                           (selectfiles, comp_session_noise, [('brain_mask','mask_files')]),\n",
    "                           (selectfiles, session_noise, [('nonbrain_mask','mask')]),\n",
    "                           (selectfiles, comp_wmcsf_noise, [('brain_mask','mask_files')]),\n",
    "                           \n",
    "                           (wmcsf_noise, comp_wmcsf_noise, [('blurred_masked_file','realigned_file')]),\n",
    "                           (session_noise, comp_session_noise, [('blurred_masked_file','realigned_file')]),\n",
    "                           (wmcsf_noise, datasink, [('blurred_masked_file','wmcsf_noise_file')]),\n",
    "                           (session_noise, datasink, [('blurred_masked_file','session_noise_file')]),\n",
    "                           (comp_wmcsf_noise, datasink, [('components_file','subject_wmcsf_comp_noise')]),\n",
    "                           (comp_session_noise, datasink, [('components_file','subject_session_comp_noise')]),\n",
    "                           \n",
    "                           (selectfiles, prep_motion, [('motion','motion_file')]),\n",
    "                           (selectfiles, identify_spikes,[('fd','fd'),('dvars','dvars')]),\n",
    "                           (prep_motion, datasink, [('leadlagderivsmot','leadlagderivsmotion'),\n",
    "                                                    ('leadlagderivssqmot','leadlagderivs_squaremotion')]),\n",
    "                           (identify_spikes, datasink, [('spike_matrix','motion_spike_matrix'), \n",
    "                                                        ('spike_timeseries','motion_spike_timeseries')]),\n",
    "                           (selectfiles, calc_globalsignal, [('func','func'),('brain_mask','mask')]),\n",
    "                           (calc_globalsignal,datasink,[('global_signal','global_signal')])\n",
    "                          ])\n",
    "create_noise_flow.base_dir = workflow_dir\n",
    "#create_noise_flow.write_graph(graph2use='flat')\n",
    "create_noise_flow.run('MultiProc', plugin_args={'n_procs': 6})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Denoising Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "substitutions = [('_subject_id_', '_'),('_filename_','')]\n",
    "datasink.inputs.substitutions = substitutions\n",
    "\n",
    "#file handling nodes\n",
    "selectfiles_template={'motion': output_dir + '/motion_parameters/{filename}_{subject_id}/rest_moco.nii.gz.par', \n",
    "                      'leadlagderivsmotion': output_dir + '/leadlagderivsmotion/{filename}_{subject_id}/derivsleadlag.txt',\n",
    "                      'global_signal': output_dir + '/global_signal/{filename}_{subject_id}/global_signal.txt',\n",
    "                      'func': output_dir + '/registered_func/{filename}_{subject_id}/norm_func.nii.gz',\n",
    "                      'session': output_dir + '/session_noise_file/{filename}_{subject_id}/blurred_masked_file.nii.gz',\n",
    "                      'wmcsf': output_dir + '/wmcsf_noise_file/{filename}_{subject_id}/blurred_masked_file.nii.gz',\n",
    "                      'spike_matrix':output_dir + '/motion_spike_matrix/{filename}_{subject_id}/spike_matrix.txt',\n",
    "                      'spike_timeseries':output_dir + '/motion_spike_timeseries/{filename}_{subject_id}/spike_timeseries.txt',\n",
    "                      'mask': output_dir + '/brain_mask/{subject_id}/whole_brain_mask.nii.gz'}\n",
    "selectfiles=Node(SelectFiles(selectfiles_template),name='selectfiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_shared_noise(motion, leadlagderivsmotion, spike_matrix, global_signal):\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from numpy import loadtxt, concatenate,ndim, expand_dims\n",
    "    from pandas import DataFrame\n",
    "    from os.path import abspath\n",
    "\n",
    "    noise_list = []\n",
    "    for file in [motion, leadlagderivsmotion, spike_matrix]:\n",
    "        mo = loadtxt(file, dtype=float, comments=None)\n",
    "        if ndim(mo)<2:\n",
    "            mo = expand_dims(mo, axis=1)\n",
    "        noise_list.append(mo)\n",
    "\n",
    "    shared_noise_data = concatenate(noise_list,axis=1)\n",
    "\n",
    "    col_names = ['noise_{0}'.format(a) for a in range(0,shared_noise_data.shape[1])] \n",
    "\n",
    "    shared_noise = DataFrame(shared_noise_data, columns=col_names)\n",
    "    shared_noise['global_signal'] = loadtxt(global_signal, dtype=float, comments=None)\n",
    "    shared_noise.to_csv('shared_noise.csv')\n",
    "    shared_noise_file = abspath('shared_noise.csv')\n",
    "    return(shared_noise_file)\n",
    "\n",
    "def voxelwise_glm(func, shared_noise_file, mask, wmcsf, session):\n",
    "    from os.path import abspath\n",
    "    from numpy import zeros, dot, transpose, sum\n",
    "    from numpy.linalg import pinv\n",
    "    from pandas import read_csv, Series\n",
    "    from nilearn.masking import apply_mask, unmask\n",
    "\n",
    "    # import data into an array that is timepoints (rows) by voxel number (columns)\n",
    "    shared_noise = read_csv(shared_noise_file, index_col=0)\n",
    "    func_data = apply_mask(func, mask)\n",
    "    wmcsf_data = apply_mask(wmcsf, mask)\n",
    "    session_data = apply_mask(session, mask)\n",
    "    coefficients = zeros((shared_noise.shape[1]+3,func_data.shape[1]))\n",
    "    resid_data = zeros(func_data.shape)\n",
    "\n",
    "    # perform voxel-wise matrix inversion\n",
    "    for x in range(0,func_data.shape[1]):\n",
    "        shared_noise['wmcsf'] = wmcsf_data[:,x]\n",
    "        shared_noise['session'] = session_data[:,x]\n",
    "        shared_noise['constant'] = 1\n",
    "        noise_mat = shared_noise.to_numpy()\n",
    "        y = func_data[:,x]\n",
    "        inv_mat = pinv(noise_mat)\n",
    "        coefficients[:,x] = dot(inv_mat,y)\n",
    "        yhat=sum(transpose(coefficients[:,x])*noise_mat,axis=1)\n",
    "        resid_data[:,x] = y - transpose(yhat)\n",
    "\n",
    "    resid_image = unmask(resid_data, mask)\n",
    "    resid_image.to_filename('residuals.nii.gz')\n",
    "\n",
    "    coeff_image = unmask(coefficients, mask)\n",
    "    coeff_image.to_filename('weights.nii.gz')\n",
    "    sample_design_df = shared_noise.to_csv('last_noise_mat.csv')\n",
    "\n",
    "    weights = abspath('weights.nii.gz')\n",
    "    sample_design_df = abspath('last_noise_mat.csv')\n",
    "    residuals = abspath('residuals.nii.gz')\n",
    "\n",
    "    return(weights,sample_design_df, residuals)\n",
    "\n",
    "def drop_high_motion_trs(in_file, brain_mask, timeseries_mask):\n",
    "    from os.path import abspath\n",
    "    from nipype import config, logging\n",
    "    config.enable_debug_mode()\n",
    "    logging.update_logging(config)\n",
    "    from nilearn.masking import apply_mask, unmask\n",
    "    import numpy as np\n",
    "    \n",
    "    spikes=np.genfromtxt(timeseries_mask).astype(int)\n",
    "    vols_to_keep = spikes==0\n",
    "    func_data = apply_mask(in_file,brain_mask)\n",
    "    func_data = func_data[vols_to_keep]\n",
    "    lomo_image = unmask(func_data,brain_mask)\n",
    "    lomo_image.to_filename('lomo_func.nii.gz')\n",
    "    out_file = abspath('lomo_func.nii.gz')\n",
    "    \n",
    "    return(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "compile_noise_mat = Node(Function(input_names=['motion', 'leadlagderivsmotion', 'spike_matrix','global_signal'],\n",
    "                                  output_names=['shared_noise_file'],\n",
    "                                  function=org_shared_noise), \n",
    "                         name='compile_noise_mat')\n",
    "\n",
    "denoise_func = Node(Function(input_names=['func','shared_noise_file','mask','wmcsf','session'], \n",
    "                             output_names=['weights','sample_design_df','residuals'],\n",
    "                             function=voxelwise_glm),\n",
    "                       name='denoise_func')\n",
    "\n",
    "# band pass filtering- all rates are in Hz (1/TR)\n",
    "bandpass = Node(Bandpass(highpass=highpass_freq,\n",
    "                         lowpass=lowpass_freq, \n",
    "                         out_file='resids_bp.nii.gz'), \n",
    "                name='bandpass')\n",
    "\n",
    "drop_himo = Node(Function(input_names=['in_file','brain_mask','timeseries_mask'], \n",
    "                          output_names=['out_file'], \n",
    "                          function=drop_high_motion_trs), \n",
    "                 name='drop_himo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "denoise_flow = Workflow(name='denoise_flow')\n",
    "denoise_flow.connect([(funcinfosource,selectfiles,[('subject_id','subject_id'),\n",
    "                                                   ('filename','filename')]),\n",
    "                      (selectfiles, denoise_func, [('mask','mask'),('func','func'),\n",
    "                                                   ('wmcsf','wmcsf'),('session','session')]),\n",
    "                      (selectfiles, compile_noise_mat, [('motion','motion'),\n",
    "                                                        ('leadlagderivsmotion','leadlagderivsmotion'), \n",
    "                                                        ('global_signal','global_signal'),\n",
    "                                                        ('spike_matrix','spike_matrix')]),\n",
    "                      (selectfiles, drop_himo, [('mask','brain_mask'),('spike_timeseries','timeseries_mask')]),\n",
    "                      (compile_noise_mat, denoise_func, [('shared_noise_file','shared_noise_file')]),\n",
    "                      (denoise_func, bandpass, [('residuals','in_file')]), \n",
    "                      (bandpass, drop_himo, [('out_file','in_file')]),\n",
    "                      \n",
    "                      (bandpass, datasink, [('out_file','full_proc_func')]),\n",
    "                      (drop_himo, datasink, [('out_file','lomo_proc_func')]),\n",
    "                      (denoise_func, datasink, [('weights','denoising_weights'),\n",
    "                                                ('sample_design_df','denoise_sample_design_df')])\n",
    "                     ])\n",
    "denoise_flow.base_dir = workflow_dir\n",
    "#denoise_flow.write_graph(graph2use='flat')\n",
    "denoise_flow.run('MultiProc', plugin_args={'n_procs': 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge files according to acquisition condition (fixation/rest or mlp/movie)\n",
    "from glob import glob\n",
    "from os import mkdir\n",
    "from nipype.interfaces.fsl import Merge\n",
    "\n",
    "merge = Merge()\n",
    "merge.inputs.dimension='t'\n",
    "\n",
    "for sub in [3000, 4000, 5000]:\n",
    "    mkdir(output_dir + '/lomo_proc_func/{0}'.format(sub))\n",
    "    for cond in ['rest','mlp']:\n",
    "        if cond=='rest':\n",
    "            out_dir = output_dir + '/lomo_proc_func/{0}/fixation'.format(sub)\n",
    "        elif cond=='mlp':\n",
    "            out_dir = output_dir + '/lomo_proc_func/{0}/video'.format(sub)\n",
    "        mkdir(out_dir)\n",
    "        files = glob(output_dir + '/lomo_proc_func/{0}*{1}/lomo_func.nii.gz'.format(cond,sub))\n",
    "        merge.inputs.in_files = files\n",
    "        merge.inputs.merged_file = out_dir + '/merged_func.nii.gz'\n",
    "        merge.run()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run hcp pipeline func processing script to put the data in HCP format\n",
    "from subprocess import check_call\n",
    "\n",
    "for sub in ['3000', '5000']:\n",
    "    for cond in ['fixation','video']:\n",
    "        check_call(['./processing_5yop_func.sh',sub,cond])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
